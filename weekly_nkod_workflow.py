#!/usr/bin/env python3
"""
T√Ωdenn√≠ NKOD scraping workflow s automatick√Ωm spu≈°tƒõn√≠m ka≈æd√© pondƒõl√≠
Vyu≈æ√≠v√° LangGraph pro orchestraci a scheduling
"""

import os
import json
import shutil
from datetime import datetime, timedelta
from typing import Dict, List, Any, TypedDict, Optional, Literal
from pathlib import Path
import schedule
import time
import logging

from langgraph.graph import StateGraph, END, START
from langgraph.checkpoint.memory import MemorySaver
from pydantic import BaseModel, Field

# Import uzl≈Ø z p≈ôedchoz√≠ho souboru
from nkod_langchain_nodes import (
    NKODConfig, 
    DataFetchNode, 
    MetadataParseNode, 
    OutputNode,
    NKODState
)
from json_diff_node import JSONDiffNode
from embedding_node import EmbeddingNode, EmbeddingConfig

# Nastaven√≠ loggingu
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('nkod_weekly.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class WeeklyNKODState(TypedDict):
    """Roz≈°√≠≈ôen√Ω stav pro t√Ωdenn√≠ workflow"""
    # Z√°kladn√≠ NKOD stav
    sparql_endpoint: str
    session_headers: Dict[str, str]
    output_dir: str
    datasets: List[str]
    processed_datasets: List[Dict[str, Any]]
    statistics: Dict[str, Any]
    limit: Optional[int]
    total_count: int
    hvd_count: int
    
    # T√Ωdenn√≠ specifick√© polo≈æky
    previous_dataset_exists: bool
    current_dataset_path: str
    previous_dataset_path: str
    backup_created: bool
    workflow_start_time: str
    workflow_status: Literal["starting", "checking", "backing_up", "scraping", "saving", "diffing", "embedding", "completed", "error"]
    error_message: Optional[str]
    
    # Diff specifick√© polo≈æky
    dataset_changes: List[Dict[str, Any]]
    changes_summary: Dict[str, Any]
    new_or_modified_datasets: List[Dict[str, Any]]
    diff_completed: bool
    diff_error: Optional[str]
    
    # Embedding specifick√© polo≈æky
    embeddings_processed: int
    embeddings_dimension: int
    vector_db_collection: str
    vector_db_type: str
    embeddings_error: Optional[str]
    vector_db_initialized: bool


class WeeklyConfig(BaseModel):
    """Konfigurace pro t√Ωdenn√≠ workflow"""
    nkod_config: NKODConfig = Field(default_factory=NKODConfig)
    output_base_dir: str = "weekly_nkod_data"
    current_dataset_name: str = "dataset_current.json"
    previous_dataset_name: str = "dataset_previous.json"
    backup_retention_days: int = 30
    enable_scheduler: bool = True
    schedule_time: str = "09:00"  # Pondƒõl√≠ v 9:00
    
    # Embedding konfigurace
    embedding_config: EmbeddingConfig = Field(default_factory=lambda: EmbeddingConfig(
        embedding_provider="huggingface",
        embedding_model="sentence-transformers/all-MiniLM-L6-v2",
        vector_db="chroma",
        collection_name="nkod_weekly_datasets",
        vector_db_config={"persist_directory": "./nkod_vector_db"}
    ))


class FileCheckNode:
    """Uzel pro kontrolu existuj√≠c√≠ch soubor≈Ø"""
    
    def __init__(self, config: WeeklyConfig):
        self.config = config
        
    def __call__(self, state: WeeklyNKODState) -> WeeklyNKODState:
        """Zkontroluje existenci p≈ôedchoz√≠ch dataset≈Ø"""
        logger.info("Kontroluji existuj√≠c√≠ datasety...")
        
        # Nastaven√≠ cest
        base_dir = Path(self.config.output_base_dir)
        current_path = base_dir / self.config.current_dataset_name
        previous_path = base_dir / self.config.previous_dataset_name
        
        # Vytvo≈ôen√≠ adres√°≈ôe pokud neexistuje
        base_dir.mkdir(exist_ok=True)
        
        # Kontrola existence souƒçasn√©ho datasetu
        previous_exists = current_path.exists()
        
        logger.info(f"Souƒçasn√Ω dataset existuje: {previous_exists}")
        if previous_exists:
            logger.info(f"Nalezen dataset: {current_path}")
        
        # Aktualizace stavu
        state.update({
            "previous_dataset_exists": previous_exists,
            "current_dataset_path": str(current_path),
            "previous_dataset_path": str(previous_path),
            "backup_created": False,
            "workflow_status": "checking"
        })
        
        return state


class BackupNode:
    """Uzel pro z√°lohov√°n√≠ p≈ôedchoz√≠ch dat"""
    
    def __init__(self, config: WeeklyConfig):
        self.config = config
        
    def __call__(self, state: WeeklyNKODState) -> WeeklyNKODState:
        """P≈ôejmenuje souƒçasn√Ω dataset na previous"""
        if not state["previous_dataset_exists"]:
            logger.info("≈Ω√°dn√Ω p≈ôedchoz√≠ dataset k z√°lohov√°n√≠")
            state["backup_created"] = True
            state["workflow_status"] = "backing_up"
            return state
            
        try:
            current_path = Path(state["current_dataset_path"])
            previous_path = Path(state["previous_dataset_path"])
            
            # Pokud u≈æ existuje previous, vytvo≈ô√≠ timestampovanou z√°lohu
            if previous_path.exists():
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                backup_path = previous_path.parent / f"dataset_backup_{timestamp}.json"
                shutil.move(str(previous_path), str(backup_path))
                logger.info(f"Star√° previous z√°loha p≈ôesunuta do: {backup_path}")
            
            # P≈ôejmenov√°n√≠ current na previous
            shutil.move(str(current_path), str(previous_path))
            logger.info(f"Dataset p≈ôejmenov√°n: {current_path} -> {previous_path}")
            
            state.update({
                "backup_created": True,
                "workflow_status": "backing_up"
            })
            
        except Exception as e:
            error_msg = f"Chyba p≈ôi z√°lohov√°n√≠: {e}"
            logger.error(error_msg)
            state.update({
                "backup_created": False,
                "workflow_status": "error",
                "error_message": error_msg
            })
            
        return state


class WeeklyScrapingNode:
    """Wrapper uzel pro spu≈°tƒõn√≠ NKOD scrapingu"""
    
    def __init__(self, config: WeeklyConfig):
        self.config = config
        self.data_fetch_node = DataFetchNode(config.nkod_config)
        self.metadata_parse_node = MetadataParseNode(config.nkod_config)
        
    def __call__(self, state: WeeklyNKODState) -> WeeklyNKODState:
        """Spust√≠ kompletn√≠ NKOD scraping"""
        try:
            logger.info("Spou≈°t√≠m NKOD scraping...")
            state["workflow_status"] = "scraping"
            
            # Konverze na z√°kladn√≠ NKOD stav
            nkod_state = NKODState(
                sparql_endpoint=state.get("sparql_endpoint", ""),
                session_headers=state.get("session_headers", {}),
                output_dir=state.get("output_dir", ""),
                datasets=state.get("datasets", []),
                processed_datasets=state.get("processed_datasets", []),
                statistics=state.get("statistics", {}),
                limit=state.get("limit"),
                total_count=state.get("total_count", 0),
                hvd_count=state.get("hvd_count", 0)
            )
            
            # Spu≈°tƒõn√≠ jednotliv√Ωch uzl≈Ø
            logger.info("1. Stahov√°n√≠ seznamu dataset≈Ø...")
            nkod_state = self.data_fetch_node(nkod_state)
            
            logger.info(f"2. Parsov√°n√≠ {len(nkod_state['datasets'])} dataset≈Ø...")
            nkod_state = self.metadata_parse_node(nkod_state)
            
            # Aktualizace hlavn√≠ho stavu
            state.update(nkod_state)
            state["workflow_status"] = "scraping"
            
            logger.info(f"Scraping dokonƒçen. Zpracov√°no {len(nkod_state['processed_datasets'])} dataset≈Ø")
            
        except Exception as e:
            error_msg = f"Chyba p≈ôi scrapingu: {e}"
            logger.error(error_msg)
            state.update({
                "workflow_status": "error",
                "error_message": error_msg
            })
            
        return state


class WeeklySaveNode:
    """Uzel pro ulo≈æen√≠ v√Ωsledk≈Ø do weekly form√°tu"""
    
    def __init__(self, config: WeeklyConfig):
        self.config = config
        
    def __call__(self, state: WeeklyNKODState) -> WeeklyNKODState:
        """Ulo≈æ√≠ v√Ωsledky jako dataset_current.json"""
        try:
            logger.info("Ukl√°d√°m v√Ωsledky...")
            state["workflow_status"] = "saving"
            
            # P≈ô√≠prava dat pro ulo≈æen√≠
            output_data = {
                "metadata": {
                    "created_at": state["workflow_start_time"],
                    "total_datasets": len(state["processed_datasets"]),
                    "hvd_datasets": len([d for d in state["processed_datasets"] if d.get("isHVD")]),
                    "scraper_version": "2.1-HVD-LangChain-Weekly",
                    "statistics": state.get("statistics", {})
                },
                "datasets": state["processed_datasets"]
            }
            
            # Ulo≈æen√≠ do current datasetu
            current_path = Path(state["current_dataset_path"])
            with open(current_path, "w", encoding="utf-8") as f:
                json.dump(output_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"V√Ωsledky ulo≈æeny do: {current_path}")
            logger.info(f"Celkem dataset≈Ø: {output_data['metadata']['total_datasets']}")
            logger.info(f"HVD dataset≈Ø: {output_data['metadata']['hvd_datasets']}")
            
            state["workflow_status"] = "saving"
            
        except Exception as e:
            error_msg = f"Chyba p≈ôi ukl√°d√°n√≠: {e}"
            logger.error(error_msg)
            state.update({
                "workflow_status": "error",
                "error_message": error_msg
            })
            
        return state


class CleanupNode:
    """Uzel pro √∫klid star√Ωch z√°loh"""
    
    def __init__(self, config: WeeklyConfig):
        self.config = config
        
    def __call__(self, state: WeeklyNKODState) -> WeeklyNKODState:
        """Vyma≈æe star√© z√°lohy podle retention policy"""
        try:
            base_dir = Path(self.config.output_base_dir)
            retention_date = datetime.now() - timedelta(days=self.config.backup_retention_days)
            
            # Najde a vyma≈æe star√© z√°lohy
            deleted_files = 0
            for backup_file in base_dir.glob("dataset_backup_*.json"):
                if backup_file.stat().st_mtime < retention_date.timestamp():
                    backup_file.unlink()
                    deleted_files += 1
                    logger.info(f"Vymaz√°na star√° z√°loha: {backup_file}")
            
            if deleted_files > 0:
                logger.info(f"Vymaz√°no {deleted_files} star√Ωch z√°loh")
            else:
                logger.info("≈Ω√°dn√© star√© z√°lohy k vymaz√°n√≠")
                
        except Exception as e:
            logger.warning(f"Chyba p≈ôi √∫klidu z√°loh: {e}")
            
        return state


def should_backup(state: WeeklyNKODState) -> Literal["backup", "scrape"]:
    """Kondicion√°ln√≠ funkce - urƒçuje zda z√°lohovat"""
    return "backup" if state["previous_dataset_exists"] else "scrape"


def workflow_successful(state: WeeklyNKODState) -> Literal["diff", "end"]:
    """Kondicion√°ln√≠ funkce - pokraƒçovat na diff nebo skonƒçit"""
    return "diff" if state["workflow_status"] == "saving" else "end"


def diff_successful(state: WeeklyNKODState) -> Literal["embedding", "end"]:
    """Kondicion√°ln√≠ funkce - pokraƒçovat na embedding nebo skonƒçit"""
    return "embedding" if state.get("diff_completed", False) else "end"


def embedding_successful(state: WeeklyNKODState) -> Literal["cleanup", "end"]:
    """Kondicion√°ln√≠ funkce - embedding √∫spƒõ≈°n√Ω"""
    return "cleanup" if state.get("vector_db_initialized", False) else "end"


def create_weekly_workflow(config: WeeklyConfig = None) -> StateGraph:
    """Vytvo≈ô√≠ LangGraph workflow pro t√Ωdenn√≠ NKOD scraping"""
    if config is None:
        config = WeeklyConfig()
    
    # Vytvo≈ôen√≠ uzl≈Ø
    file_check_node = FileCheckNode(config)
    backup_node = BackupNode(config)
    scraping_node = WeeklyScrapingNode(config)
    save_node = WeeklySaveNode(config)
    diff_node = JSONDiffNode(config)
    embedding_node = EmbeddingNode(config.embedding_config)
    cleanup_node = CleanupNode(config)
    
    # Vytvo≈ôen√≠ grafu
    workflow = StateGraph(WeeklyNKODState)
    
    # P≈ôid√°n√≠ uzl≈Ø
    workflow.add_node("file_check", file_check_node)
    workflow.add_node("backup", backup_node)
    workflow.add_node("scrape", scraping_node)
    workflow.add_node("save", save_node)
    workflow.add_node("diff", diff_node)
    workflow.add_node("embedding", embedding_node)
    workflow.add_node("cleanup", cleanup_node)
    
    # Definice p≈ôechod≈Ø
    workflow.add_edge(START, "file_check")
    workflow.add_conditional_edges(
        "file_check",
        should_backup,
        {
            "backup": "backup",
            "scrape": "scrape"
        }
    )
    workflow.add_edge("backup", "scrape")
    workflow.add_edge("scrape", "save")
    workflow.add_conditional_edges(
        "save",
        workflow_successful,
        {
            "diff": "diff",
            "end": END
        }
    )
    workflow.add_conditional_edges(
        "diff",
        diff_successful,
        {
            "embedding": "embedding",
            "end": END
        }
    )
    workflow.add_conditional_edges(
        "embedding",
        embedding_successful,
        {
            "cleanup": "cleanup",
            "end": END
        }
    )
    workflow.add_edge("cleanup", END)
    
    return workflow


def run_weekly_workflow(config: WeeklyConfig = None) -> WeeklyNKODState:
    """Spust√≠ t√Ωdenn√≠ workflow jednou"""
    if config is None:
        config = WeeklyConfig()
    
    # Vytvo≈ôen√≠ workflow
    workflow = create_weekly_workflow(config)
    app = workflow.compile(checkpointer=MemorySaver())
    
    # Konfigurace pro thread
    thread_config = {"configurable": {"thread_id": f"weekly_nkod_{datetime.now().strftime('%Y%m%d_%H%M%S')}"}}
    
    # Inicializaƒçn√≠ stav
    initial_state = WeeklyNKODState(
        sparql_endpoint="",
        session_headers={},
        output_dir="",
        datasets=[],
        processed_datasets=[],
        statistics={},
        limit=config.nkod_config.limit,
        total_count=0,
        hvd_count=0,
        previous_dataset_exists=False,
        current_dataset_path="",
        previous_dataset_path="",
        backup_created=False,
        workflow_start_time=datetime.now().isoformat(),
        workflow_status="starting",
        error_message=None,
        dataset_changes=[],
        changes_summary={},
        new_or_modified_datasets=[],
        diff_completed=False,
        diff_error=None,
        embeddings_processed=0,
        embeddings_dimension=0,
        vector_db_collection="",
        vector_db_type="",
        embeddings_error=None,
        vector_db_initialized=False
    )
    
    logger.info("=== Spou≈°t√≠m t√Ωdenn√≠ NKOD workflow ===")
    
    try:
        # Spu≈°tƒõn√≠ workflow
        final_state = app.invoke(initial_state, thread_config)
        
        if final_state.get("vector_db_initialized", False):
            logger.info("‚úÖ T√Ωdenn√≠ workflow √∫spƒõ≈°nƒõ dokonƒçen!")
            
            # Vyp√≠≈°e statistiky diff
            summary = final_state.get("changes_summary", {})
            if summary:
                logger.info(f"üìä Zmƒõny v datech:")
                logger.info(f"  - Nov√© datasety: {summary.get('new_datasets', 0)}")
                logger.info(f"  - Zmƒõnƒõn√© datasety: {summary.get('modified_datasets', 0)}")
                logger.info(f"  - Smazan√© datasety: {summary.get('deleted_datasets', 0)}")
                logger.info(f"  - Vr√°ceno nov√Ωch/zmƒõnƒõn√Ωch: {len(final_state.get('new_or_modified_datasets', []))}")
            
            # Vyp√≠≈°e statistiky embeddingu
            logger.info(f"üîÆ Embedding statistiky:")
            logger.info(f"  - Zpracov√°no embedding≈Ø: {final_state.get('embeddings_processed', 0)}")
            logger.info(f"  - Dimenze vektoru: {final_state.get('embeddings_dimension', 0)}")
            logger.info(f"  - Vektorov√° DB: {final_state.get('vector_db_type', 'N/A')}")
            logger.info(f"  - Kolekce: {final_state.get('vector_db_collection', 'N/A')}")
            
        elif final_state["workflow_status"] == "error":
            logger.error(f"‚ùå Workflow selhal: {final_state.get('error_message', 'Nezn√°m√° chyba')}")
        elif final_state.get("diff_error"):
            logger.error(f"‚ùå Diff selhal: {final_state.get('diff_error', 'Nezn√°m√° chyba')}")
        elif final_state.get("embeddings_error"):
            logger.error(f"‚ùå Embedding selhal: {final_state.get('embeddings_error', 'Nezn√°m√° chyba')}")
        
        return final_state
        
    except Exception as e:
        logger.error(f"‚ùå Kritick√° chyba ve workflow: {e}")
        raise


class WeeklyScheduler:
    """Scheduler pro automatick√© spou≈°tƒõn√≠ ka≈æd√© pondƒõl√≠"""
    
    def __init__(self, config: WeeklyConfig = None):
        self.config = config or WeeklyConfig()
        self.running = False
        
    def scheduled_job(self):
        """√öloha spou≈°tƒõn√° scheduleru"""
        logger.info("üïí Spou≈°t√≠ se napl√°novan√Ω t√Ωdenn√≠ scraping...")
        try:
            run_weekly_workflow(self.config)
        except Exception as e:
            logger.error(f"Chyba v napl√°novan√© √∫loze: {e}")
    
    def start_scheduler(self):
        """Spust√≠ scheduler"""
        if not self.config.enable_scheduler:
            logger.info("Scheduler je zak√°z√°n v konfiguraci")
            return
            
        # Napl√°nov√°n√≠ na ka≈æd√© pondƒõl√≠
        schedule.every().monday.at(self.config.schedule_time).do(self.scheduled_job)
        
        logger.info(f"üìÖ Scheduler nastaven na ka≈æd√© pondƒõl√≠ v {self.config.schedule_time}")
        logger.info("Scheduler bƒõ≈æ√≠... (Ctrl+C pro ukonƒçen√≠)")
        
        self.running = True
        try:
            while self.running:
                schedule.run_pending()
                time.sleep(60)  # Kontrola ka≈ædou minutu
        except KeyboardInterrupt:
            logger.info("Scheduler ukonƒçen u≈æivatelem")
        finally:
            self.running = False
    
    def stop_scheduler(self):
        """Zastav√≠ scheduler"""
        self.running = False
        schedule.clear()


def main():
    """Hlavn√≠ funkce s CLI rozhran√≠m"""
    import sys
    
    config = WeeklyConfig()
    
    if len(sys.argv) > 1:
        command = sys.argv[1].lower()
        
        if command == "run":
            # Jednor√°zov√© spu≈°tƒõn√≠
            print("Spou≈°t√≠m t√Ωdenn√≠ workflow jednor√°zovƒõ...")
            run_weekly_workflow(config)
            
        elif command == "schedule":
            # Spu≈°tƒõn√≠ scheduleru
            scheduler = WeeklyScheduler(config)
            scheduler.start_scheduler()
            
        elif command == "test":
            # Test s omezen√Ωm poƒçtem dataset≈Ø
            config.nkod_config.limit = 5
            print("Testovac√≠ spu≈°tƒõn√≠ s 5 datasety...")
            run_weekly_workflow(config)
            
        elif command == "help":
            print("T√Ωdenn√≠ NKOD Scraper - LangGraph workflow")
            print("Pou≈æit√≠:")
            print("  python weekly_nkod_workflow.py run       # Jednor√°zov√© spu≈°tƒõn√≠")
            print("  python weekly_nkod_workflow.py schedule  # Spust√≠ scheduler (ka≈æd√© pondƒõl√≠)")
            print("  python weekly_nkod_workflow.py test      # Test s 5 datasety")
            print("  python weekly_nkod_workflow.py help      # Tato n√°povƒõda")
            
        else:
            print(f"Nezn√°m√Ω p≈ô√≠kaz: {command}")
            print("Pou≈æijte 'python weekly_nkod_workflow.py help' pro n√°povƒõdu")
            sys.exit(1)
    else:
        # V√Ωchoz√≠ chov√°n√≠ - jednor√°zov√© spu≈°tƒõn√≠
        print("Spou≈°t√≠m t√Ωdenn√≠ workflow (pou≈æijte 'help' pro dal≈°√≠ mo≈ænosti)...")
        run_weekly_workflow(config)


if __name__ == "__main__":
    main()