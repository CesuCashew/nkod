# ğŸ“š NKOD LangGraph Workflow - KompletnÃ­ Tutorial

## ğŸ¯ Co je to NKOD Workflow?

NKOD (NÃ¡rodnÃ­ katalog otevÅ™enÃ½ch dat) Workflow je automatizovanÃ½ systÃ©m postavenÃ½ na **LangGraph**, kterÃ½:

- ğŸŒ **Stahuje** metadata z ÄeskÃ©ho NKOD (data.gov.cz)
- ğŸ” **PorovnÃ¡vÃ¡** zmÄ›ny mezi bÄ›hy
- ğŸ”® **VytvÃ¡Å™Ã­ embeddingy** jen pro novÃ¡/zmÄ›nÄ›nÃ¡ data  
- ğŸ’¾ **UklÃ¡dÃ¡** do vektorovÃ© databÃ¡ze pro podobnostnÃ­ vyhledÃ¡vÃ¡nÃ­
- ğŸ“… **BÄ›Å¾Ã­ automaticky** kaÅ¾dÃ© pondÄ›lÃ­ nebo manuÃ¡lnÄ›

---

## ğŸš€ RychlÃ½ start

### 1. Instalace zÃ¡vislostÃ­

```bash
# PÅ™ejdÄ›te do adresÃ¡Å™e
cd /Users/filiphirt/Desktop/nkod

# Nainstalujte poÅ¾adovanÃ© balÃ­Äky
pip3 install -r requirements.txt

# Nainstalujte dodateÄnÃ© balÃ­Äky pro embeddingy
pip3 install sentence-transformers chromadb
```

### 2. PrvnÃ­ spuÅ¡tÄ›nÃ­

```bash
# Test s 3 datasety (rychlÃ©)
python3 complete_nkod_workflow.py test

# ManuÃ¡lnÃ­ spuÅ¡tÄ›nÃ­ s 10 datasety
python3 complete_nkod_workflow.py run 10

# ZobrazenÃ­ nÃ¡povÄ›dy
python3 complete_nkod_workflow.py help
```

---

## ğŸ“– PodrobnÃ½ nÃ¡vod

### ğŸ”§ Konfigurace

Workflow pouÅ¾Ã­vÃ¡ tÅ™Ã­du `CompleteWorkflowConfig` s tÄ›mito hlavnÃ­mi nastavenÃ­mi:

```python
# ZÃ¡kladnÃ­ nastavenÃ­
output_base_dir = "complete_nkod_data"          # VÃ½stupnÃ­ sloÅ¾ka
backup_retention_days = 30                      # Jak dlouho uchovÃ¡vat zÃ¡lohy

# Scheduling
enable_scheduler = True                          # Povolit automatickÃ© spouÅ¡tÄ›nÃ­
schedule_time = "09:00"                         # ÄŒas spuÅ¡tÄ›nÃ­ (pondÄ›lÃ­)
schedule_day = "monday"                         # Den tÃ½dne

# Embedding nastavenÃ­
embedding_provider = "huggingface"              # Ollama nebo HuggingFace
embedding_model = "sentence-transformers/all-MiniLM-L6-v2"
vector_db = "chroma"                            # ChromaDB, Qdrant, nebo Weaviate
```

### ğŸ“ Struktura souborÅ¯

Po spuÅ¡tÄ›nÃ­ workflow vytvoÅ™Ã­ nÃ¡sledujÃ­cÃ­ strukturu:

```
complete_nkod_data/
â”œâ”€â”€ dataset_current.json         # NejnovÄ›jÅ¡Ã­ data
â”œâ”€â”€ dataset_previous.json        # Data z pÅ™edchozÃ­ho bÄ›hu
â”œâ”€â”€ dataset_backup_*.json        # StarÅ¡Ã­ zÃ¡lohy (timestamp)
â”œâ”€â”€ workflow_results/            # VÃ½sledky jednotlivÃ½ch bÄ›hÅ¯
â”‚   â”œâ”€â”€ result_nkod_20250723_103125.json
â”‚   â””â”€â”€ result_nkod_20250723_103153.json
â””â”€â”€ complete_nkod_vector_db/     # ChromaDB vektorovÃ¡ databÃ¡ze
    â”œâ”€â”€ chroma.sqlite3
    â””â”€â”€ [embeddings data]
```

---

## ğŸ® ZpÅ¯soby spuÅ¡tÄ›nÃ­

### 1. ğŸ§ª TestovacÃ­ reÅ¾im
```bash
python3 complete_nkod_workflow.py test
```
- StÃ¡hne **3 datasety** (rychlÃ© pro testovÃ¡nÃ­)
- IdeÃ¡lnÃ­ pro ovÄ›Å™enÃ­, Å¾e vÅ¡e funguje

### 2. ğŸ–±ï¸ ManuÃ¡lnÃ­ spuÅ¡tÄ›nÃ­
```bash
# Bez limitu (vÅ¡echna data - mÅ¯Å¾e trvat dlouho!)
python3 complete_nkod_workflow.py run

# S limitem (doporuÄeno)
python3 complete_nkod_workflow.py run 50
python3 complete_nkod_workflow.py run 100
```

### 3. â° AutomatickÃ½ scheduler
```bash
python3 complete_nkod_workflow.py schedule
```
- BÄ›Å¾Ã­ kontinuÃ¡lnÄ›
- SpustÃ­ workflow kaÅ¾dÃ© **pondÄ›lÃ­ v 9:00**
- UkonÄÃ­te pomocÃ­ `Ctrl+C`

---

## ğŸ” Jak workflow funguje

### ğŸ“Š FÃ¡ze workflow

1. **ğŸš€ Trigger** - Inicializace, vytvoÅ™enÃ­ ID workflow
2. **ğŸ“ File Management** - ZÃ¡lohovÃ¡nÃ­ pÅ™edchozÃ­ch dat
3. **ğŸŒ Data Scraping** - StahovÃ¡nÃ­ z NKOD pomocÃ­ SPARQL
4. **ğŸ’¾ Data Saving** - UloÅ¾enÃ­ do JSON souboru
5. **ğŸ” Diffing** - PorovnÃ¡nÃ­ s pÅ™edchozÃ­mi daty
6. **ğŸ”® Embedding** - VytvoÅ™enÃ­ vektorÅ¯ jen pro zmÄ›ny
7. **ğŸ§¹ Cleanup** - Ãšklid starÃ½ch zÃ¡loh
8. **ğŸ Finalization** - VytvoÅ™enÃ­ reportu

### ğŸ¤– LangGraph konstrukce

```python
# StateGraph s kondicionÃ¡lnÃ­mi pÅ™echody
workflow = StateGraph(CompleteWorkflowState)

# Uzly
workflow.add_node("trigger", trigger_node)
workflow.add_node("file_management", file_mgmt_node)
# ... dalÅ¡Ã­ uzly

# KondicionÃ¡lnÃ­ routing
workflow.add_conditional_edges(
    "diffing",
    should_embed,  # Funkce rozhodovÃ¡nÃ­
    {
        "embedding": "embedding",    # Jsou-li zmÄ›ny â†’ embedding
        "cleanup": "cleanup"         # Nejsou zmÄ›ny â†’ pÅ™eskoÄit
    }
)
```

---

## ğŸ“Š VÃ½sledky a monitoring

### ğŸ“‹ Workflow Results

KaÅ¾dÃ½ bÄ›h vytvoÅ™Ã­ soubor s vÃ½sledky:

```json
{
  "workflow_id": "nkod_20250723_103153",
  "trigger_type": "manual",
  "start_time": "2025-07-23T10:31:25.358244",
  "end_time": "2025-07-23T10:31:26.550968", 
  "duration_seconds": 1.192724,
  "status": "completed",
  "datasets_processed": 5,
  "changes_detected": 2,
  "embeddings_created": 2,
  "errors": []
}
```

### ğŸ“Š Dataset metadata

KaÅ¾dÃ½ dataset obsahuje:

```json
{
  "metadata": {
    "workflow_id": "nkod_20250723_103153",
    "created_at": "2025-07-23T10:31:53.193849",
    "trigger_type": "manual",
    "total_datasets": 5,
    "hvd_datasets": 5,
    "scraper_version": "2.1-HVD-LangChain-Complete"
  },
  "datasets": [
    {
      "uri": "https://data.gov.cz/zdroj/datovÃ©-sady/...",
      "title": "NÃ¡zev datovÃ© sady",
      "description": "Popis datasetu...",
      "keywords": ["klÃ­ÄovÃ¡", "slova"],
      "isHVD": true,
      "harvested_at": "2025-07-23T10:31:53.785318"
    }
  ]
}
```

### ğŸ“ˆ LogovÃ¡nÃ­

Workflow loguje do nÄ›kolika mÃ­st:

1. **Konzole** - PrÅ¯bÄ›Å¾nÃ© informace bÄ›hem bÄ›hu
2. **complete_nkod_workflow.log** - KompletnÃ­ log soubor
3. **Workflow results** - JSON souhrny jednotlivÃ½ch bÄ›hÅ¯

---

## ğŸ”® PrÃ¡ce s embeddingy

### ğŸ—„ï¸ VektorovÃ¡ databÃ¡ze

Workflow automaticky:
- VytvÃ¡Å™Ã­ **384-dimenzionÃ¡lnÃ­** vektory (sentence-transformers)
- UklÃ¡dÃ¡ do **ChromaDB** kolekce `complete_nkod_datasets`
- Indexuje podle **ID**, **URL**, nebo **title hash**

### ğŸ” VyhledÃ¡vÃ¡nÃ­ (pÅ™Ã­klad pouÅ¾itÃ­)

```python
from embedding_node import EmbeddingNode, EmbeddingConfig

# Inicializace
config = EmbeddingConfig()
embedding_node = EmbeddingNode(config)

# VyhledÃ¡nÃ­ podobnÃ½ch datasetÅ¯
results = embedding_node.search_similar(
    "dopravnÃ­ statistiky praha", 
    limit=5
)

for result in results:
    print(f"Podobnost: {result['score']}")
    print(f"NÃ¡zev: {result['metadata']['title']}")
    print(f"URL: {result['metadata']['url']}")
```

---

## âš™ï¸ PokroÄilÃ¡ konfigurace

### ğŸ”§ VlastnÃ­ konfigurace

```python
from complete_nkod_workflow import CompleteWorkflowConfig, CompleteNKODWorkflow
from embedding_node import EmbeddingConfig

# VlastnÃ­ konfigurace
config = CompleteWorkflowConfig(
    # ZÃ¡kladnÃ­
    output_base_dir="my_nkod_data",
    backup_retention_days=60,
    
    # Scheduling  
    schedule_time="08:00",
    schedule_day="tuesday",
    
    # Embedding
    embedding_config=EmbeddingConfig(
        embedding_provider="ollama",
        embedding_model="nomic-embed-text", 
        vector_db="qdrant",  # Pokud mÃ¡te Qdrant server
        vector_db_config={"url": "localhost", "port": 6333}
    )
)

# SpuÅ¡tÄ›nÃ­ s vlastnÃ­ konfiguracÃ­
workflow = CompleteNKODWorkflow(config)
result = workflow.run_manual(limit=20)
```

### ğŸ—„ï¸ JinÃ© vektorovÃ© databÃ¡ze

#### Qdrant
```bash
# SpuÅ¡tÄ›nÃ­ Qdrant serveru
docker run -p 6333:6333 qdrant/qdrant

# Konfigurace
embedding_config = EmbeddingConfig(
    vector_db="qdrant",
    vector_db_config={"url": "localhost", "port": 6333}
)
```

#### Weaviate
```bash
# SpuÅ¡tÄ›nÃ­ Weaviate
docker run -p 8080:8080 weaviate/weaviate:latest

# Konfigurace  
embedding_config = EmbeddingConfig(
    vector_db="weaviate",
    vector_db_config={"url": "http://localhost:8080"}
)
```

---

## ğŸ› Å˜eÅ¡enÃ­ problÃ©mÅ¯

### âŒ ÄŒastÃ© chyby a Å™eÅ¡enÃ­

#### 1. **Import chyby**
```bash
# Chyba: ModuleNotFoundError: No module named 'sentence_transformers'
pip3 install sentence-transformers

# Chyba: ModuleNotFoundError: No module named 'chromadb' 
pip3 install chromadb
```

#### 2. **SPARQL endpoint nedostupnÃ½**
```
Chyba pÅ™i dotazu SPARQL: 502 Server Error: Bad Gateway
```
**Å˜eÅ¡enÃ­**: NKOD server je doÄasnÄ› nedostupnÃ½, zkuste pozdÄ›ji.

#### 3. **ChromaDB metadata chyby**
```
Expected metadata value to be a str, int, float, bool, or None, got [...] which is a list
```
**Å˜eÅ¡enÃ­**: Automaticky vyÅ™eÅ¡eno - workflow pÅ™evÃ¡dÃ­ seznamy na stringy.

#### 4. **Nedostatek mÃ­sta na disku**
```
OSError: [Errno 28] No space left on device
```
**Å˜eÅ¡enÃ­**: 
- VyÄistÄ›te starÃ© zÃ¡lohy: `rm complete_nkod_data/dataset_backup_*.json`
- SniÅ¾te `backup_retention_days`

### ğŸ”§ Debug reÅ¾im

```python
import logging

# ZapnutÃ­ debug loggingu
logging.getLogger().setLevel(logging.DEBUG)

# Nebo spusÅ¥te s verbose logovÃ¡nÃ­m
python3 complete_nkod_workflow.py run 5 --verbose
```

---

## ğŸ“ˆ Optimalizace a tipy

### âš¡ VÃ½konovÃ© tipy

1. **ZaÄnÄ›te s malÃ½mi limity** (10-50 datasetÅ¯) pro testovÃ¡nÃ­
2. **PouÅ¾Ã­vejte scheduler** pro pravilnÃ© aktualizace
3. **Monitorujte disk space** - embeddingy zabÃ­rajÃ­ mÃ­sto
4. **PravidelnÄ› ÄistÄ›te zÃ¡lohy** starÅ¡Ã­ neÅ¾ 30 dnÃ­

### ğŸ¯ DoporuÄenÃ© workflow

```bash
# 1. PrvotnÃ­ test
python3 complete_nkod_workflow.py test

# 2. MalÃ½ produkÄnÃ­ bÄ›h  
python3 complete_nkod_workflow.py run 100

# 3. SpuÅ¡tÄ›nÃ­ scheduleru pro automatizaci
python3 complete_nkod_workflow.py schedule
```

### ğŸ“Š Monitoring produkce

```bash
# SledovÃ¡nÃ­ logÅ¯
tail -f complete_nkod_workflow.log

# Kontrola vÃ½sledkÅ¯
ls -la complete_nkod_data/workflow_results/

# Velikost vektorovÃ© databÃ¡ze
du -sh complete_nkod_data/complete_nkod_vector_db/
```

---

## ğŸ”— Integrace s dalÅ¡Ã­mi systÃ©my

### ğŸ“¡ API endpoint (pÅ™Ã­klad)

```python
from flask import Flask, jsonify
from embedding_node import EmbeddingNode, EmbeddingConfig

app = Flask(__name__)
embedding_node = EmbeddingNode(EmbeddingConfig())

@app.route('/search/<query>')
def search_datasets(query):
    results = embedding_node.search_similar(query, limit=10)
    return jsonify({
        'query': query,
        'results': results,
        'count': len(results)
    })

if __name__ == '__main__':
    app.run(port=5000)
```

### ğŸ“Š Jupyter Notebook analÃ½za

```python
import json
import pandas as pd
from pathlib import Path

# NaÄtenÃ­ dat
with open('complete_nkod_data/dataset_current.json', 'r') as f:
    data = json.load(f)

# Konverze na DataFrame
df = pd.DataFrame(data['datasets'])

# AnalÃ½za
print(f"Celkem datasetÅ¯: {len(df)}")
print(f"HVD datasetÅ¯: {df['isHVD'].sum()}")
print(f"NejÄastÄ›jÅ¡Ã­ klÃ­ÄovÃ¡ slova: {df['keywords'].explode().value_counts().head()}")
```

---

## ğŸ“ PokroÄilÃ© pouÅ¾itÃ­

### ğŸ”€ VlastnÃ­ uzly

```python
class CustomProcessingNode:
    """VlastnÃ­ uzel pro specifickÃ© zpracovÃ¡nÃ­"""
    
    def __init__(self, config):
        self.config = config
        
    def __call__(self, state):
        # VlastnÃ­ logika
        logger.info("ğŸ”¥ SpouÅ¡tÃ­m vlastnÃ­ zpracovÃ¡nÃ­...")
        
        # ZpracovÃ¡nÃ­ dat
        processed_data = self.process_datasets(state['processed_datasets'])
        
        # Aktualizace stavu
        state['custom_processing_completed'] = True
        state['custom_results'] = processed_data
        
        return state
        
    def process_datasets(self, datasets):
        # Implementace vlastnÃ­ logiky
        return datasets

# Integrace do workflow
workflow.add_node("custom", CustomProcessingNode(config))
workflow.add_edge("embedding", "custom")
workflow.add_edge("custom", "cleanup")
```

### ğŸ“Š VlastnÃ­ metriky

```python
class MetricsCollector:
    """SbÄ›r vlastnÃ­ch metrik"""
    
    def collect_metrics(self, state):
        metrics = {
            'processing_time': time.time() - start_time,
            'memory_usage': psutil.Process().memory_info().rss,
            'datasets_per_second': len(datasets) / duration,
            'hvd_ratio': hvd_count / total_count
        }
        
        # UloÅ¾enÃ­ metrik
        with open('metrics.json', 'w') as f:
            json.dump(metrics, f)
            
        return metrics
```

---

## ğŸ“ Podpora a FAQ

### â“ ÄŒasto kladenÃ© otÃ¡zky

**Q: Jak dlouho trvÃ¡ zpracovÃ¡nÃ­ vÅ¡ech dat?**
A: ZÃ¡visÃ­ na poÄtu datasetÅ¯. ~1000 datasetÅ¯ = 5-10 minut.

**Q: Mohu zmÄ›nit embedding model?**
A: Ano, v konfiguraci nastavte `embedding_model` na jakÃ½koli HuggingFace model.

**Q: Jak mohu vyhledÃ¡vat v datech?**
A: PouÅ¾ijte `embedding_node.search_similar("vÃ¡Å¡ dotaz")` nebo vytvoÅ™te vlastnÃ­ API.

**Q: BÄ›Å¾Ã­ workflow i bez internetu?**
A: Ne, potÅ™ebuje pÅ™Ã­stup k data.gov.cz a HuggingFace modelÅ¯m.

**Q: Mohu upravit Äas schedulingu?**
A: Ano, zmÄ›Åˆte `schedule_time` a `schedule_day` v konfiguraci.

### ğŸ†˜ Podpora

- **GitHub Issues**: Reportujte chyby a poÅ¾adavky
- **Dokumentace**: Tento tutorial a komentÃ¡Å™e v kÃ³du
- **Logy**: VÅ¾dy zkontrolujte `complete_nkod_workflow.log`

---

## ğŸ‰ ZÃ¡vÄ›r

Gratulujeme! NynÃ­ mÃ¡te kompletnÃ­ automatizovanÃ½ systÃ©m pro:

- âœ… **StahovÃ¡nÃ­** NKOD dat
- âœ… **Detekci zmÄ›n** mezi bÄ›hy  
- âœ… **VytvÃ¡Å™enÃ­ embeddingÅ¯** pro podobnostnÃ­ vyhledÃ¡vÃ¡nÃ­
- âœ… **AutomatickÃ© spouÅ¡tÄ›nÃ­** kaÅ¾dÃ© pondÄ›lÃ­
- âœ… **KompletnÃ­ monitoring** a logovÃ¡nÃ­

**ZaÄnÄ›te s:** `python3 complete_nkod_workflow.py test`

**ProdukÄnÃ­ nasazenÃ­:** `python3 complete_nkod_workflow.py schedule`

---

*ğŸ“ Naposledy aktualizovÃ¡no: 23. Äervence 2025*  
*ğŸš€ NKOD LangGraph Workflow v2.1*