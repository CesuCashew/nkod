#!/usr/bin/env python3
"""
KompletnÃ­ NKOD LangGraph workflow
Integruje vÅ¡echny komponenty do jednoho automatizovanÃ©ho systÃ©mu
"""

import os
import json
import shutil
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Literal, TypedDict
from pathlib import Path
import schedule
import time
import logging
from dataclasses import dataclass

# LangGraph imports
from langgraph.graph import StateGraph, END, START
from langgraph.checkpoint.memory import MemorySaver
from pydantic import BaseModel, Field

# Import vÅ¡ech uzlÅ¯
from nkod_langchain_nodes import (
    NKODConfig, 
    DataFetchNode, 
    MetadataParseNode, 
    OutputNode,
    NKODState
)
from json_diff_node import JSONDiffNode
from embedding_node import EmbeddingNode, EmbeddingConfig

# NastavenÃ­ loggingu
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('complete_nkod_workflow.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class CompleteWorkflowState(TypedDict):
    """KompletnÃ­ stav pro celÃ½ workflow"""
    # ZÃ¡kladnÃ­ NKOD stav
    sparql_endpoint: str
    session_headers: Dict[str, str]
    output_dir: str
    datasets: List[str]
    processed_datasets: List[Dict[str, Any]]
    statistics: Dict[str, Any]
    limit: Optional[int]
    total_count: int
    hvd_count: int
    
    # Workflow management
    workflow_start_time: str
    workflow_status: Literal["starting", "checking", "backing_up", "scraping", "saving", "diffing", "embedding", "cleanup", "completed", "error"]
    workflow_id: str
    trigger_type: Literal["manual", "scheduled"]
    error_message: Optional[str]
    
    # File management
    previous_dataset_exists: bool
    current_dataset_path: str
    previous_dataset_path: str
    backup_created: bool
    files_managed: List[str]
    
    # Diff processing
    dataset_changes: List[Dict[str, Any]]
    changes_summary: Dict[str, Any]
    new_or_modified_datasets: List[Dict[str, Any]]
    diff_completed: bool
    diff_error: Optional[str]
    
    # Embedding processing
    embeddings_processed: int
    embeddings_dimension: int
    vector_db_collection: str
    vector_db_type: str
    embeddings_error: Optional[str]
    vector_db_initialized: bool
    
    # Cleanup tracking
    old_backups_removed: int
    cleanup_completed: bool


class CompleteWorkflowConfig(BaseModel):
    """KompletnÃ­ konfigurace workflow"""
    # NKOD scraping
    nkod_config: NKODConfig = Field(default_factory=NKODConfig)
    
    # File management
    output_base_dir: str = "complete_nkod_data"
    current_dataset_name: str = "dataset_current.json"
    previous_dataset_name: str = "dataset_previous.json"
    backup_retention_days: int = 30
    
    # Scheduling
    enable_scheduler: bool = True
    schedule_time: str = "09:00"  # PondÄ›lÃ­ v 9:00
    schedule_day: str = "monday"
    
    # Embedding
    embedding_config: EmbeddingConfig = Field(default_factory=lambda: EmbeddingConfig(
        embedding_provider="huggingface",
        embedding_model="sentence-transformers/all-MiniLM-L6-v2",
        vector_db="chroma",
        collection_name="complete_nkod_datasets",
        vector_db_config={"persist_directory": "./complete_nkod_vector_db"}
    ))
    
    # Workflow settings
    max_workflow_duration_minutes: int = 120
    retry_attempts: int = 3
    
    class Config:
        arbitrary_types_allowed = True


@dataclass
class WorkflowResult:
    """VÃ½sledek workflow pro reporting"""
    workflow_id: str
    trigger_type: str
    start_time: datetime
    end_time: datetime
    duration_seconds: float
    status: str
    datasets_processed: int
    changes_detected: int
    embeddings_created: int
    errors: List[str]
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "workflow_id": self.workflow_id,
            "trigger_type": self.trigger_type,
            "start_time": self.start_time.isoformat(),
            "end_time": self.end_time.isoformat(),
            "duration_seconds": self.duration_seconds,
            "status": self.status,
            "datasets_processed": self.datasets_processed,
            "changes_detected": self.changes_detected,
            "embeddings_created": self.embeddings_created,
            "errors": self.errors
        }


class WorkflowTriggerNode:
    """VstupnÃ­ uzel - inicializace workflow"""
    
    def __init__(self, config: CompleteWorkflowConfig):
        self.config = config
        
    def __call__(self, state: CompleteWorkflowState) -> CompleteWorkflowState:
        workflow_id = f"nkod_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        trigger_type = state.get("trigger_type", "manual")
        
        logger.info(f"ğŸš€ SpouÅ¡tÃ­m NKOD workflow: {workflow_id}")
        logger.info(f"ğŸ“‹ Typ spuÅ¡tÄ›nÃ­: {trigger_type}")
        
        # VytvoÅ™enÃ­ vÃ½stupnÃ­ch adresÃ¡Å™Å¯
        base_dir = Path(self.config.output_base_dir)
        base_dir.mkdir(exist_ok=True)
        
        # Aktualizace stavu
        state.update({
            "workflow_id": workflow_id,
            "workflow_start_time": datetime.now().isoformat(),
            "workflow_status": "starting",
            "trigger_type": trigger_type,
            "current_dataset_path": str(base_dir / self.config.current_dataset_name),
            "previous_dataset_path": str(base_dir / self.config.previous_dataset_name),
            "files_managed": [],
            "error_message": None
        })
        
        return state


class FileManagementNode:
    """Uzel pro sprÃ¡vu souborÅ¯ - kontrola a zÃ¡lohovÃ¡nÃ­"""
    
    def __init__(self, config: CompleteWorkflowConfig):
        self.config = config
        
    def __call__(self, state: CompleteWorkflowState) -> CompleteWorkflowState:
        logger.info("ğŸ“ SprÃ¡va souborÅ¯...")
        
        try:
            current_path = Path(state["current_dataset_path"])
            previous_path = Path(state["previous_dataset_path"])
            
            # Kontrola existence souÄasnÃ©ho datasetu
            previous_exists = current_path.exists()
            files_managed = []
            
            if previous_exists:
                logger.info(f"ğŸ“„ Nalezen souÄasnÃ½ dataset: {current_path}")
                
                # Pokud uÅ¾ existuje previous, vytvoÅ™Ã­ timestampovanou zÃ¡lohu
                if previous_path.exists():
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    backup_path = previous_path.parent / f"dataset_backup_{timestamp}.json"
                    shutil.move(str(previous_path), str(backup_path))
                    files_managed.append(str(backup_path))
                    logger.info(f"ğŸ—‚ï¸ StarÃ¡ zÃ¡loha pÅ™esunuta: {backup_path}")
                
                # PÅ™ejmenovÃ¡nÃ­ current na previous
                shutil.move(str(current_path), str(previous_path))
                files_managed.append(str(previous_path))
                logger.info(f"ğŸ”„ Dataset pÅ™ejmenovÃ¡n: current â†’ previous")
                
                state["backup_created"] = True
            else:
                logger.info("ğŸ“„ Å½Ã¡dnÃ½ pÅ™edchozÃ­ dataset nenalezen")
                state["backup_created"] = False
            
            state.update({
                "previous_dataset_exists": previous_exists,
                "files_managed": files_managed,
                "workflow_status": "checking"
            })
            
        except Exception as e:
            error_msg = f"Chyba pÅ™i sprÃ¡vÄ› souborÅ¯: {e}"
            logger.error(error_msg)
            state.update({
                "workflow_status": "error",
                "error_message": error_msg
            })
            
        return state


class DataScrapingNode:
    """Uzel pro stahovÃ¡nÃ­ a parsovÃ¡nÃ­ NKOD dat"""
    
    def __init__(self, config: CompleteWorkflowConfig):
        self.config = config
        self.data_fetch_node = DataFetchNode(config.nkod_config)
        self.metadata_parse_node = MetadataParseNode(config.nkod_config)
        
    def __call__(self, state: CompleteWorkflowState) -> CompleteWorkflowState:
        logger.info("ğŸŒ StahovÃ¡nÃ­ NKOD dat...")
        
        try:
            state["workflow_status"] = "scraping"
            
            # Konverze na zÃ¡kladnÃ­ NKOD stav
            nkod_state = NKODState(
                sparql_endpoint=state.get("sparql_endpoint", ""),
                session_headers=state.get("session_headers", {}),
                output_dir=state.get("output_dir", ""),
                datasets=state.get("datasets", []),
                processed_datasets=state.get("processed_datasets", []),
                statistics=state.get("statistics", {}),
                limit=state.get("limit"),
                total_count=state.get("total_count", 0),
                hvd_count=state.get("hvd_count", 0)
            )
            
            # 1. StahovÃ¡nÃ­ seznamu datasetÅ¯
            logger.info("ğŸ“Š ZÃ­skÃ¡vÃ¡nÃ­ seznamu datasetÅ¯...")
            nkod_state = self.data_fetch_node(nkod_state)
            logger.info(f"âœ… Nalezeno {len(nkod_state['datasets'])} datasetÅ¯")
            
            # 2. ParsovÃ¡nÃ­ metadat
            logger.info(f"ğŸ” ParsovÃ¡nÃ­ {len(nkod_state['datasets'])} datasetÅ¯...")
            nkod_state = self.metadata_parse_node(nkod_state)
            logger.info(f"âœ… ZpracovÃ¡no {len(nkod_state['processed_datasets'])} datasetÅ¯")
            
            # Aktualizace hlavnÃ­ho stavu
            state.update(nkod_state)
            
        except Exception as e:
            error_msg = f"Chyba pÅ™i stahovÃ¡nÃ­ dat: {e}"
            logger.error(error_msg)
            state.update({
                "workflow_status": "error",
                "error_message": error_msg
            })
            
        return state


class DataSavingNode:
    """Uzel pro uloÅ¾enÃ­ dat do JSON souboru"""
    
    def __init__(self, config: CompleteWorkflowConfig):
        self.config = config
        
    def __call__(self, state: CompleteWorkflowState) -> CompleteWorkflowState:
        logger.info("ğŸ’¾ UklÃ¡dÃ¡nÃ­ dat...")
        
        try:
            # PÅ™Ã­prava dat pro uloÅ¾enÃ­
            output_data = {
                "metadata": {
                    "workflow_id": state["workflow_id"],
                    "created_at": state["workflow_start_time"],
                    "trigger_type": state["trigger_type"],
                    "total_datasets": len(state["processed_datasets"]),
                    "hvd_datasets": len([d for d in state["processed_datasets"] if d.get("isHVD")]),
                    "scraper_version": "2.1-HVD-LangChain-Complete",
                    "statistics": state.get("statistics", {})
                },
                "datasets": state["processed_datasets"]
            }
            
            # UloÅ¾enÃ­ do current datasetu
            current_path = Path(state["current_dataset_path"])
            with open(current_path, "w", encoding="utf-8") as f:
                json.dump(output_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"âœ… Data uloÅ¾ena: {current_path}")
            logger.info(f"ğŸ“Š Celkem datasetÅ¯: {output_data['metadata']['total_datasets']}")
            logger.info(f"ğŸ·ï¸ HVD datasetÅ¯: {output_data['metadata']['hvd_datasets']}")
            
            state["workflow_status"] = "saving"
            
        except Exception as e:
            error_msg = f"Chyba pÅ™i uklÃ¡dÃ¡nÃ­ dat: {e}"
            logger.error(error_msg)
            state.update({
                "workflow_status": "error",
                "error_message": error_msg
            })
            
        return state


class DiffProcessingNode:
    """Uzel pro porovnÃ¡nÃ­ dat - wrapper nad JSONDiffNode"""
    
    def __init__(self, config: CompleteWorkflowConfig):
        self.config = config
        self.diff_node = JSONDiffNode(config)
        
    def __call__(self, state: CompleteWorkflowState) -> CompleteWorkflowState:
        logger.info("ğŸ” PorovnÃ¡vÃ¡nÃ­ dat...")
        
        try:
            # SpuÅ¡tÄ›nÃ­ diff uzlu
            diff_result = self.diff_node(state)
            
            # Aktualizace stavu
            state.update(diff_result)
            
            if state.get("diff_completed", False):
                summary = state.get("changes_summary", {})
                logger.info("âœ… PorovnÃ¡nÃ­ dokonÄeno:")
                logger.info(f"  ğŸ“ˆ NovÃ© datasety: {summary.get('new_datasets', 0)}")
                logger.info(f"  ğŸ“ ZmÄ›nÄ›nÃ© datasety: {summary.get('modified_datasets', 0)}")
                logger.info(f"  ğŸ“‰ SmazanÃ© datasety: {summary.get('deleted_datasets', 0)}")
                logger.info(f"  ğŸ¯ K embedovÃ¡nÃ­: {len(state.get('new_or_modified_datasets', []))}")
            
        except Exception as e:
            error_msg = f"Chyba pÅ™i porovnÃ¡vÃ¡nÃ­ dat: {e}"
            logger.error(error_msg)
            state.update({
                "diff_error": error_msg,
                "diff_completed": False
            })
            
        return state


class EmbeddingProcessingNode:
    """Uzel pro vytvÃ¡Å™enÃ­ embeddingÅ¯ - wrapper nad EmbeddingNode"""
    
    def __init__(self, config: CompleteWorkflowConfig):
        self.config = config
        self.embedding_node = EmbeddingNode(config.embedding_config)
        
    def __call__(self, state: CompleteWorkflowState) -> CompleteWorkflowState:
        logger.info("ğŸ”® VytvÃ¡Å™enÃ­ embeddingÅ¯...")
        
        try:
            # SpuÅ¡tÄ›nÃ­ embedding uzlu
            embedding_result = self.embedding_node(state)
            
            # Aktualizace stavu
            state.update(embedding_result)
            
            if state.get("vector_db_initialized", False):
                logger.info("âœ… Embeddingy vytvoÅ™eny:")
                logger.info(f"  ğŸ§® ZpracovÃ¡no: {state.get('embeddings_processed', 0)}")
                logger.info(f"  ğŸ“ Dimenze: {state.get('embeddings_dimension', 0)}")
                logger.info(f"  ğŸ—„ï¸ DatabÃ¡ze: {state.get('vector_db_type', 'N/A')}")
                logger.info(f"  ğŸ“š Kolekce: {state.get('vector_db_collection', 'N/A')}")
                
                state["workflow_status"] = "embedding"
            
        except Exception as e:
            error_msg = f"Chyba pÅ™i vytvÃ¡Å™enÃ­ embeddingÅ¯: {e}"
            logger.error(error_msg)
            state.update({
                "embeddings_error": error_msg,
                "vector_db_initialized": False
            })
            
        return state


class CleanupNode:
    """Uzel pro Ãºklid starÃ½ch souborÅ¯"""
    
    def __init__(self, config: CompleteWorkflowConfig):
        self.config = config
        
    def __call__(self, state: CompleteWorkflowState) -> CompleteWorkflowState:
        logger.info("ğŸ§¹ Ãšklid starÃ½ch souborÅ¯...")
        
        try:
            base_dir = Path(self.config.output_base_dir)
            retention_date = datetime.now() - timedelta(days=self.config.backup_retention_days)
            
            # Najde a vymaÅ¾e starÃ© zÃ¡lohy
            deleted_files = 0
            for backup_file in base_dir.glob("dataset_backup_*.json"):
                if backup_file.stat().st_mtime < retention_date.timestamp():
                    backup_file.unlink()
                    deleted_files += 1
                    logger.info(f"ğŸ—‘ï¸ VymazÃ¡na starÃ¡ zÃ¡loha: {backup_file.name}")
            
            if deleted_files > 0:
                logger.info(f"âœ… VymazÃ¡no {deleted_files} starÃ½ch zÃ¡loh")
            else:
                logger.info("â„¹ï¸ Å½Ã¡dnÃ© starÃ© zÃ¡lohy k vymazÃ¡nÃ­")
            
            state.update({
                "old_backups_removed": deleted_files,
                "cleanup_completed": True,
                "workflow_status": "cleanup"
            })
            
        except Exception as e:
            error_msg = f"Chyba pÅ™i Ãºklidu: {e}"
            logger.error(error_msg)
            state.update({
                "cleanup_completed": False,
                "error_message": error_msg
            })
            
        return state


class WorkflowFinalizationNode:
    """FinalizaÄnÃ­ uzel - dokonÄenÃ­ workflow"""
    
    def __init__(self, config: CompleteWorkflowConfig):
        self.config = config
        
    def __call__(self, state: CompleteWorkflowState) -> CompleteWorkflowState:
        logger.info("ğŸ DokonÄovÃ¡nÃ­ workflow...")
        
        # VÃ½poÄet statistik
        start_time = datetime.fromisoformat(state["workflow_start_time"])
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()
        
        # VytvoÅ™enÃ­ vÃ½sledku
        result = WorkflowResult(
            workflow_id=state["workflow_id"],
            trigger_type=state["trigger_type"],
            start_time=start_time,
            end_time=end_time,
            duration_seconds=duration,
            status=state.get("workflow_status", "unknown"),
            datasets_processed=len(state.get("processed_datasets", [])),
            changes_detected=len(state.get("new_or_modified_datasets", [])),
            embeddings_created=state.get("embeddings_processed", 0),
            errors=[e for e in [
                state.get("error_message"),
                state.get("diff_error"),
                state.get("embeddings_error")
            ] if e]
        )
        
        # UloÅ¾enÃ­ vÃ½sledku
        results_dir = Path(self.config.output_base_dir) / "workflow_results"
        results_dir.mkdir(exist_ok=True)
        
        result_file = results_dir / f"result_{state['workflow_id']}.json"
        with open(result_file, "w", encoding="utf-8") as f:
            json.dump(result.to_dict(), f, ensure_ascii=False, indent=2)
        
        # FinÃ¡lnÃ­ report
        if result.errors:
            logger.error(f"âŒ Workflow dokonÄen s chybami: {state['workflow_id']}")
            for error in result.errors:
                logger.error(f"  ğŸš¨ {error}")
            state["workflow_status"] = "error"
        else:
            logger.info(f"âœ… Workflow ÃºspÄ›Å¡nÄ› dokonÄen: {state['workflow_id']}")
            logger.info(f"â±ï¸ TrvÃ¡nÃ­: {duration:.1f}s")
            logger.info(f"ğŸ“Š ZpracovÃ¡no datasetÅ¯: {result.datasets_processed}")
            logger.info(f"ğŸ”„ ZmÄ›n detekovÃ¡no: {result.changes_detected}")
            logger.info(f"ğŸ”® EmbeddingÅ¯ vytvoÅ™eno: {result.embeddings_created}")
            state["workflow_status"] = "completed"
        
        return state


# KondicionÃ¡lnÃ­ funkce pro routing
def should_backup(state: CompleteWorkflowState) -> Literal["scraping", "error"]:
    """Rozhoduje o pokraÄovÃ¡nÃ­ workflow"""
    return "scraping" if state["workflow_status"] == "checking" else "error"


def should_diff(state: CompleteWorkflowState) -> Literal["diffing", "error"]:
    """Rozhoduje o spuÅ¡tÄ›nÃ­ diff"""
    return "diffing" if state["workflow_status"] == "saving" else "error"


def should_embed(state: CompleteWorkflowState) -> Literal["embedding", "cleanup"]:
    """Rozhoduje o spuÅ¡tÄ›nÃ­ embedding"""
    if state.get("diff_completed", False) and len(state.get("new_or_modified_datasets", [])) > 0:
        return "embedding"
    else:
        return "cleanup"


def should_finalize(state: CompleteWorkflowState) -> Literal["finalization", "error"]:
    """Rozhoduje o finalizaci"""
    return "finalization" if not state.get("error_message") else "error"


class CompleteNKODWorkflow:
    """HlavnÃ­ tÅ™Ã­da pro kompletnÃ­ NKOD workflow"""
    
    def __init__(self, config: CompleteWorkflowConfig = None):
        self.config = config or CompleteWorkflowConfig()
        self.workflow_graph = None
        self.app = None
        self._build_workflow()
        
    def _build_workflow(self):
        """SestavÃ­ LangGraph workflow"""
        logger.info("ğŸ—ï¸ Sestavuji workflow graf...")
        
        # VytvoÅ™enÃ­ uzlÅ¯
        trigger_node = WorkflowTriggerNode(self.config)
        file_mgmt_node = FileManagementNode(self.config)
        scraping_node = DataScrapingNode(self.config)
        saving_node = DataSavingNode(self.config)
        diff_node = DiffProcessingNode(self.config)
        embedding_node = EmbeddingProcessingNode(self.config)
        cleanup_node = CleanupNode(self.config)
        finalization_node = WorkflowFinalizationNode(self.config)
        
        # VytvoÅ™enÃ­ grafu
        self.workflow_graph = StateGraph(CompleteWorkflowState)
        
        # PÅ™idÃ¡nÃ­ uzlÅ¯
        self.workflow_graph.add_node("trigger", trigger_node)
        self.workflow_graph.add_node("file_management", file_mgmt_node)
        self.workflow_graph.add_node("scraping", scraping_node)
        self.workflow_graph.add_node("saving", saving_node)
        self.workflow_graph.add_node("diffing", diff_node)
        self.workflow_graph.add_node("embedding", embedding_node)
        self.workflow_graph.add_node("cleanup", cleanup_node)
        self.workflow_graph.add_node("finalization", finalization_node)
        
        # Definice pÅ™echodÅ¯
        self.workflow_graph.add_edge(START, "trigger")
        self.workflow_graph.add_edge("trigger", "file_management")
        
        self.workflow_graph.add_conditional_edges(
            "file_management",
            should_backup,
            {
                "scraping": "scraping",
                "error": "finalization"
            }
        )
        
        self.workflow_graph.add_edge("scraping", "saving")
        
        self.workflow_graph.add_conditional_edges(
            "saving",
            should_diff,
            {
                "diffing": "diffing",
                "error": "finalization"
            }
        )
        
        self.workflow_graph.add_conditional_edges(
            "diffing",
            should_embed,
            {
                "embedding": "embedding",
                "cleanup": "cleanup"
            }
        )
        
        self.workflow_graph.add_edge("embedding", "cleanup")
        
        self.workflow_graph.add_conditional_edges(
            "cleanup",
            should_finalize,
            {
                "finalization": "finalization",
                "error": "finalization"
            }
        )
        
        self.workflow_graph.add_edge("finalization", END)
        
        # Kompilace grafu
        self.app = self.workflow_graph.compile(checkpointer=MemorySaver())
        logger.info("âœ… Workflow graf sestaven")
        
    def run_manual(self, limit: int = None) -> WorkflowResult:
        """SpustÃ­ workflow ruÄnÄ›"""
        logger.info("ğŸ–±ï¸ ManuÃ¡lnÃ­ spuÅ¡tÄ›nÃ­ workflow...")
        
        # Aktualizace konfigurace
        if limit:
            self.config.nkod_config.limit = limit
            
        # InicializaÄnÃ­ stav
        initial_state = CompleteWorkflowState(
            # NKOD zÃ¡kladnÃ­ stav
            sparql_endpoint="",
            session_headers={},
            output_dir="",
            datasets=[],
            processed_datasets=[],
            statistics={},
            limit=limit,
            total_count=0,
            hvd_count=0,
            
            # Workflow management
            workflow_start_time="",
            workflow_status="starting",
            workflow_id="",
            trigger_type="manual",
            error_message=None,
            
            # File management
            previous_dataset_exists=False,
            current_dataset_path="",
            previous_dataset_path="",
            backup_created=False,
            files_managed=[],
            
            # Diff processing
            dataset_changes=[],
            changes_summary={},
            new_or_modified_datasets=[],
            diff_completed=False,
            diff_error=None,
            
            # Embedding processing
            embeddings_processed=0,
            embeddings_dimension=0,
            vector_db_collection="",
            vector_db_type="",
            embeddings_error=None,
            vector_db_initialized=False,
            
            # Cleanup
            old_backups_removed=0,
            cleanup_completed=False
        )
        
        # SpuÅ¡tÄ›nÃ­ workflow
        thread_config = {"configurable": {"thread_id": f"manual_{datetime.now().strftime('%Y%m%d_%H%M%S')}"}}
        
        try:
            final_state = self.app.invoke(initial_state, thread_config)
            return self._extract_result(final_state)
        except Exception as e:
            logger.error(f"âŒ KritickÃ¡ chyba workflow: {e}")
            raise
            
    def run_scheduled(self) -> WorkflowResult:
        """SpustÃ­ workflow jako naplÃ¡novanÃ½ Ãºkol"""
        logger.info("â° NaplÃ¡novanÃ© spuÅ¡tÄ›nÃ­ workflow...")
        
        # PodobnÃ© jako manual, ale s trigger_type="scheduled"
        initial_state = CompleteWorkflowState(
            # ... stejnÃ© jako manual ...
            trigger_type="scheduled",
            # ... zbytek ...
            sparql_endpoint="",
            session_headers={},
            output_dir="",
            datasets=[],
            processed_datasets=[],
            statistics={},
            limit=self.config.nkod_config.limit,
            total_count=0,
            hvd_count=0,
            workflow_start_time="",
            workflow_status="starting",
            workflow_id="",
            error_message=None,
            previous_dataset_exists=False,
            current_dataset_path="",
            previous_dataset_path="",
            backup_created=False,
            files_managed=[],
            dataset_changes=[],
            changes_summary={},
            new_or_modified_datasets=[],
            diff_completed=False,
            diff_error=None,
            embeddings_processed=0,
            embeddings_dimension=0,
            vector_db_collection="",
            vector_db_type="",
            embeddings_error=None,
            vector_db_initialized=False,
            old_backups_removed=0,
            cleanup_completed=False
        )
        
        thread_config = {"configurable": {"thread_id": f"scheduled_{datetime.now().strftime('%Y%m%d_%H%M%S')}"}}
        
        try:
            final_state = self.app.invoke(initial_state, thread_config)
            return self._extract_result(final_state)
        except Exception as e:
            logger.error(f"âŒ KritickÃ¡ chyba naplÃ¡novanÃ©ho workflow: {e}")
            raise
            
    def _extract_result(self, state: CompleteWorkflowState) -> WorkflowResult:
        """Extrahuje vÃ½sledek z finÃ¡lnÃ­ho stavu"""
        start_time = datetime.fromisoformat(state["workflow_start_time"])
        end_time = datetime.now()
        
        return WorkflowResult(
            workflow_id=state["workflow_id"],
            trigger_type=state["trigger_type"],
            start_time=start_time,
            end_time=end_time,
            duration_seconds=(end_time - start_time).total_seconds(),
            status=state["workflow_status"],
            datasets_processed=len(state.get("processed_datasets", [])),
            changes_detected=len(state.get("new_or_modified_datasets", [])),
            embeddings_created=state.get("embeddings_processed", 0),
            errors=[e for e in [
                state.get("error_message"),
                state.get("diff_error"), 
                state.get("embeddings_error")
            ] if e]
        )
        
    def start_scheduler(self):
        """SpustÃ­ automatickÃ½ scheduler"""
        if not self.config.enable_scheduler:
            logger.info("ğŸ“… Scheduler je zakÃ¡zÃ¡n v konfiguraci")
            return
            
        # NaplÃ¡novÃ¡nÃ­ na urÄenÃ½ den a Äas
        if self.config.schedule_day.lower() == "monday":
            schedule.every().monday.at(self.config.schedule_time).do(self._scheduled_job)
        elif self.config.schedule_day.lower() == "tuesday":
            schedule.every().tuesday.at(self.config.schedule_time).do(self._scheduled_job)
        # ... dalÅ¡Ã­ dny podle potÅ™eby
        else:
            logger.error(f"âŒ NepodporovanÃ½ den: {self.config.schedule_day}")
            return
            
        logger.info(f"ğŸ“… Scheduler nastaven na kaÅ¾dÃ© {self.config.schedule_day} v {self.config.schedule_time}")
        logger.info("ğŸ”„ Scheduler bÄ›Å¾Ã­... (Ctrl+C pro ukonÄenÃ­)")
        
        try:
            while True:
                schedule.run_pending()
                time.sleep(60)  # Kontrola kaÅ¾dou minutu
        except KeyboardInterrupt:
            logger.info("â¹ï¸ Scheduler ukonÄen uÅ¾ivatelem")
        finally:
            schedule.clear()
            
    def _scheduled_job(self):
        """NaplÃ¡novanÃ¡ Ãºloha"""
        try:
            logger.info("â° SpouÅ¡tÃ­m naplÃ¡novanÃ½ NKOD workflow...")
            result = self.run_scheduled()
            if result.status == "completed":
                logger.info(f"âœ… NaplÃ¡novanÃ½ workflow ÃºspÄ›Å¡nÄ› dokonÄen: {result.workflow_id}")
            else:
                logger.error(f"âŒ NaplÃ¡novanÃ½ workflow selhal: {result.workflow_id}")
        except Exception as e:
            logger.error(f"ğŸ’¥ KritickÃ¡ chyba v naplÃ¡novanÃ© Ãºloze: {e}")


def main():
    """HlavnÃ­ funkce s CLI rozhranÃ­m"""
    import sys
    
    config = CompleteWorkflowConfig()
    workflow = CompleteNKODWorkflow(config)
    
    if len(sys.argv) > 1:
        command = sys.argv[1].lower()
        
        if command == "run":
            # ManuÃ¡lnÃ­ spuÅ¡tÄ›nÃ­
            limit = None
            if len(sys.argv) > 2:
                try:
                    limit = int(sys.argv[2])
                except ValueError:
                    print("âŒ NeplatnÃ½ limit, pouÅ¾ije se vÃ½chozÃ­")
            
            print("ğŸš€ SpouÅ¡tÃ­m kompletnÃ­ NKOD workflow ruÄnÄ›...")
            result = workflow.run_manual(limit)
            print(f"\nğŸ“‹ VÃ½sledek workflow:")
            print(f"  ID: {result.workflow_id}")
            print(f"  Status: {result.status}")
            print(f"  TrvÃ¡nÃ­: {result.duration_seconds:.1f}s")
            print(f"  ZpracovÃ¡no: {result.datasets_processed} datasetÅ¯")
            print(f"  ZmÄ›ny: {result.changes_detected}")
            print(f"  Embeddingy: {result.embeddings_created}")
            
        elif command == "schedule":
            # SpuÅ¡tÄ›nÃ­ scheduleru
            print("ğŸ“… SpouÅ¡tÃ­m automatickÃ½ scheduler...")
            workflow.start_scheduler()
            
        elif command == "test":
            # Test s omezenÃ½m poÄtem
            print("ğŸ§ª TestovacÃ­ spuÅ¡tÄ›nÃ­ s 3 datasety...")
            result = workflow.run_manual(3)
            print(f"âœ… Test dokonÄen: {result.status}")
            
        elif command == "help":
            print("ğŸ”§ KompletnÃ­ NKOD LangGraph Workflow")
            print("PouÅ¾itÃ­:")
            print(f"  {sys.argv[0]} run [limit]    # ManuÃ¡lnÃ­ spuÅ¡tÄ›nÃ­")
            print(f"  {sys.argv[0]} schedule       # AutomatickÃ½ scheduler")
            print(f"  {sys.argv[0]} test           # Test s 3 datasety")
            print(f"  {sys.argv[0]} help           # Tato nÃ¡povÄ›da")
            print()
            print("Funkce:")
            print("  ğŸ”„ AutomatickÃ© stahovÃ¡nÃ­ a parsovÃ¡nÃ­ NKOD dat")
            print("  ğŸ“Š Detekce zmÄ›n mezi bÄ›hy")
            print("  ğŸ”® AutomatickÃ© vytvÃ¡Å™enÃ­ embeddingÅ¯ pro novÃ¡/zmÄ›nÄ›nÃ¡ data")
            print("  ğŸ“… SchedulovanÃ½ bÄ›h kaÅ¾dÃ© pondÄ›lÃ­ v 9:00")  
            print("  ğŸ—‚ï¸ SprÃ¡va zÃ¡loh a Ãºklid starÃ½ch souborÅ¯")
            print("  ğŸ“‹ KompletnÃ­ logovÃ¡nÃ­ a reporting")
            
        else:
            print(f"âŒ NeznÃ¡mÃ½ pÅ™Ã­kaz: {command}")
            print(f"PouÅ¾ijte '{sys.argv[0]} help' pro nÃ¡povÄ›du")
            sys.exit(1)
    else:
        # VÃ½chozÃ­ chovÃ¡nÃ­ - manuÃ¡lnÃ­ spuÅ¡tÄ›nÃ­
        print("ğŸš€ SpouÅ¡tÃ­m kompletnÃ­ NKOD workflow...")
        result = workflow.run_manual()
        print(f"ğŸ“Š Workflow dokonÄen: {result.status}")


if __name__ == "__main__":
    main()