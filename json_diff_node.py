#!/usr/bin/env python3
"""
JSON Diff Node pro LangGraph workflow
PorovnÃ¡vÃ¡ dataset_previous.json a dataset_current.json a vracÃ­ rozdÃ­ly
"""

import json
import hashlib
from pathlib import Path
from typing import Dict, List, Any, Optional, Literal, Set, Tuple
from dataclasses import dataclass
import logging

logger = logging.getLogger(__name__)


@dataclass
class DatasetChange:
    """Reprezentuje zmÄ›nu v datasetu"""
    change_type: Literal["new", "modified", "deleted"]
    dataset_id: str
    dataset_uri: str
    previous_data: Optional[Dict[str, Any]] = None
    current_data: Optional[Dict[str, Any]] = None
    changed_fields: List[str] = None
    
    def to_dict(self) -> Dict[str, Any]:
        """Konverze na slovnÃ­k"""
        return {
            "change_type": self.change_type,
            "dataset_id": self.dataset_id,
            "dataset_uri": self.dataset_uri,
            "previous_data": self.previous_data,
            "current_data": self.current_data,
            "changed_fields": self.changed_fields or []
        }


class JSONDiffAnalyzer:
    """TÅ™Ã­da pro efektivnÃ­ porovnÃ¡nÃ­ JSON datasetÅ¯"""
    
    def __init__(self):
        self.ignored_fields = {
            "harvested_at",
            "scraper_version"
        }
    
    def _get_dataset_id(self, dataset: Dict[str, Any]) -> str:
        """ZÃ­skÃ¡ unikÃ¡tnÃ­ ID datasetu pro porovnÃ¡nÃ­"""
        # Priorita: identifier > URI > title hash
        if dataset.get("identifier"):
            return f"id_{dataset['identifier']}"
        elif dataset.get("uri"):
            return f"uri_{dataset['uri']}"
        elif dataset.get("title"):
            title_hash = hashlib.md5(dataset["title"].encode()).hexdigest()[:8]
            return f"title_{title_hash}"
        else:
            # Fallback - hash celÃ©ho objektu
            obj_str = json.dumps(dataset, sort_keys=True)
            obj_hash = hashlib.md5(obj_str.encode()).hexdigest()[:8]
            return f"hash_{obj_hash}"
    
    def _normalize_dataset(self, dataset: Dict[str, Any]) -> Dict[str, Any]:
        """Normalizuje dataset pro porovnÃ¡nÃ­ (odstranÃ­ ignorovanÃ© pole)"""
        normalized = {k: v for k, v in dataset.items() 
                     if k not in self.ignored_fields}
        
        # SeÅ™adÃ­ seznamy pro konzistentnÃ­ porovnÃ¡nÃ­
        for key in ["keywords", "themes", "applicableLegislation"]:
            if key in normalized and isinstance(normalized[key], list):
                normalized[key] = sorted(normalized[key])
        
        return normalized
    
    def _calculate_content_hash(self, dataset: Dict[str, Any]) -> str:
        """VypoÄÃ­tÃ¡ hash obsahu datasetu pro detekci zmÄ›n"""
        normalized = self._normalize_dataset(dataset)
        content_str = json.dumps(normalized, sort_keys=True, ensure_ascii=False)
        return hashlib.sha256(content_str.encode()).hexdigest()
    
    def _find_changed_fields(self, prev_data: Dict[str, Any], curr_data: Dict[str, Any]) -> List[str]:
        """Najde konkrÃ©tnÃ­ pole, kterÃ¡ se zmÄ›nila"""
        changed_fields = []
        prev_normalized = self._normalize_dataset(prev_data)
        curr_normalized = self._normalize_dataset(curr_data)
        
        # Najde vÅ¡echny klÃ­Äe z obou objektÅ¯
        all_keys = set(prev_normalized.keys()) | set(curr_normalized.keys())
        
        for key in all_keys:
            prev_val = prev_normalized.get(key)
            curr_val = curr_normalized.get(key)
            
            # PorovnÃ¡nÃ­ hodnot
            if prev_val != curr_val:
                changed_fields.append(key)
        
        return sorted(changed_fields)
    
    def compare_datasets(self, previous_datasets: List[Dict[str, Any]], 
                        current_datasets: List[Dict[str, Any]]) -> List[DatasetChange]:
        """PorovnÃ¡ dva seznamy datasetÅ¯ a vrÃ¡tÃ­ zmÄ›ny"""
        changes = []
        
        # VytvoÅ™Ã­ indexy pro rychlÃ© vyhledÃ¡vÃ¡nÃ­
        prev_index = {}
        curr_index = {}
        
        # IndexovÃ¡nÃ­ pÅ™edchozÃ­ch datasetÅ¯
        for dataset in previous_datasets:
            dataset_id = self._get_dataset_id(dataset)
            content_hash = self._calculate_content_hash(dataset)
            prev_index[dataset_id] = {
                "data": dataset,
                "hash": content_hash
            }
        
        # IndexovÃ¡nÃ­ souÄasnÃ½ch datasetÅ¯
        for dataset in current_datasets:
            dataset_id = self._get_dataset_id(dataset)
            content_hash = self._calculate_content_hash(dataset)
            curr_index[dataset_id] = {
                "data": dataset,
                "hash": content_hash
            }
        
        # Najde novÃ© a zmÄ›nÄ›nÃ© datasety
        for dataset_id, curr_info in curr_index.items():
            curr_data = curr_info["data"]
            curr_hash = curr_info["hash"]
            
            if dataset_id not in prev_index:
                # NovÃ½ dataset
                changes.append(DatasetChange(
                    change_type="new",
                    dataset_id=dataset_id,
                    dataset_uri=curr_data.get("uri", ""),
                    current_data=curr_data
                ))
            else:
                # ExistujÃ­cÃ­ dataset - kontrola zmÄ›n
                prev_info = prev_index[dataset_id]
                prev_data = prev_info["data"]
                prev_hash = prev_info["hash"]
                
                if curr_hash != prev_hash:
                    # Dataset se zmÄ›nil
                    changed_fields = self._find_changed_fields(prev_data, curr_data)
                    changes.append(DatasetChange(
                        change_type="modified",
                        dataset_id=dataset_id,
                        dataset_uri=curr_data.get("uri", ""),
                        previous_data=prev_data,
                        current_data=curr_data,
                        changed_fields=changed_fields
                    ))
        
        # Najde smazanÃ© datasety
        for dataset_id, prev_info in prev_index.items():
            if dataset_id not in curr_index:
                prev_data = prev_info["data"]
                changes.append(DatasetChange(
                    change_type="deleted",
                    dataset_id=dataset_id,
                    dataset_uri=prev_data.get("uri", ""),
                    previous_data=prev_data
                ))
        
        return changes


class JSONDiffNode:
    """LangGraph uzel pro porovnÃ¡nÃ­ JSON souborÅ¯"""
    
    def __init__(self, config=None):
        self.config = config
        self.analyzer = JSONDiffAnalyzer()
        
    def load_json_file(self, file_path: str) -> Optional[Dict[str, Any]]:
        """NaÄte JSON soubor"""
        try:
            path = Path(file_path)
            if not path.exists():
                logger.warning(f"Soubor neexistuje: {file_path}")
                return None
                
            with open(path, "r", encoding="utf-8") as f:
                data = json.load(f)
                logger.info(f"NaÄten soubor: {file_path}")
                return data
        except Exception as e:
            logger.error(f"Chyba pÅ™i naÄÃ­tÃ¡nÃ­ {file_path}: {e}")
            return None
    
    def __call__(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """HlavnÃ­ funkce uzlu pro porovnÃ¡nÃ­ JSON souborÅ¯"""
        logger.info("ğŸ” SpouÅ¡tÃ­m porovnÃ¡nÃ­ JSON souborÅ¯...")
        
        try:
            # NaÄte soubory
            previous_path = state.get("previous_dataset_path")
            current_path = state.get("current_dataset_path")
            
            if not previous_path or not current_path:
                logger.error("ChybÃ­ cesty k souborÅ¯m pro porovnÃ¡nÃ­")
                state.update({
                    "diff_error": "Missing file paths",
                    "dataset_changes": [],
                    "changes_summary": {}
                })
                return state
            
            # NaÄte data
            previous_data = self.load_json_file(previous_path)
            current_data = self.load_json_file(current_path)
            
            if previous_data is None:
                logger.info("PÅ™edchozÃ­ soubor neexistuje - vÅ¡echny souÄasnÃ© datasety jsou novÃ©")
                if current_data and "datasets" in current_data:
                    # VÅ¡echny souÄasnÃ© datasety jsou novÃ©
                    changes = []
                    for dataset in current_data["datasets"]:
                        dataset_id = self.analyzer._get_dataset_id(dataset)
                        changes.append(DatasetChange(
                            change_type="new",
                            dataset_id=dataset_id,
                            dataset_uri=dataset.get("uri", ""),
                            current_data=dataset
                        ))
                else:
                    changes = []
            else:
                if current_data is None:
                    logger.error("SouÄasnÃ½ soubor neexistuje")
                    state.update({
                        "diff_error": "Current file not found",
                        "dataset_changes": [],
                        "changes_summary": {}
                    })
                    return state
                
                # Provede porovnÃ¡nÃ­
                prev_datasets = previous_data.get("datasets", [])
                curr_datasets = current_data.get("datasets", [])
                
                logger.info(f"PorovnÃ¡vÃ¡m {len(prev_datasets)} pÅ™edchozÃ­ch vs {len(curr_datasets)} souÄasnÃ½ch datasetÅ¯")
                changes = self.analyzer.compare_datasets(prev_datasets, curr_datasets)
            
            # VytvoÅ™Ã­ statistiky
            changes_summary = {
                "total_changes": len(changes),
                "new_datasets": len([c for c in changes if c.change_type == "new"]),
                "modified_datasets": len([c for c in changes if c.change_type == "modified"]),
                "deleted_datasets": len([c for c in changes if c.change_type == "deleted"])
            }
            
            # Konverze na slovnÃ­ky pro JSON serialization
            changes_as_dicts = [change.to_dict() for change in changes]
            
            # Filtruje jen novÃ© a zmÄ›nÄ›nÃ© zÃ¡znamy (podle poÅ¾adavku)
            new_or_modified = [
                change.current_data for change in changes 
                if change.change_type in ["new", "modified"] and change.current_data
            ]
            
            logger.info(f"âœ… PorovnÃ¡nÃ­ dokonÄeno:")
            logger.info(f"  - NovÃ© datasety: {changes_summary['new_datasets']}")
            logger.info(f"  - ZmÄ›nÄ›nÃ© datasety: {changes_summary['modified_datasets']}")  
            logger.info(f"  - SmazanÃ© datasety: {changes_summary['deleted_datasets']}")
            
            # Aktualizace stavu
            state.update({
                "dataset_changes": changes_as_dicts,
                "changes_summary": changes_summary,
                "new_or_modified_datasets": new_or_modified,
                "diff_completed": True,
                "workflow_status": "diffing"
            })
            
        except Exception as e:
            error_msg = f"Chyba pÅ™i porovnÃ¡nÃ­ JSON: {e}"
            logger.error(error_msg)
            state.update({
                "diff_error": error_msg,
                "dataset_changes": [],
                "changes_summary": {},
                "diff_completed": False
            })
        
        return state


# RozÅ¡Ã­Å™enÃ­ stavu pro diff funkcionalitu
class ExtendedWeeklyNKODState(dict):
    """RozÅ¡Ã­Å™enÃ½ stav obsahujÃ­cÃ­ diff informace"""
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        # PÅ™idÃ¡ novÃ© klÃ­Äe pro diff
        self.setdefault("dataset_changes", [])
        self.setdefault("changes_summary", {})
        self.setdefault("new_or_modified_datasets", [])
        self.setdefault("diff_completed", False)
        self.setdefault("diff_error", None)


def create_test_data():
    """VytvoÅ™Ã­ testovacÃ­ data pro demo"""
    previous_data = {
        "metadata": {
            "created_at": "2025-01-15T09:00:00",
            "total_datasets": 2,
            "scraper_version": "2.1-test"
        },
        "datasets": [
            {
                "uri": "https://data.gov.cz/dataset/1",
                "identifier": "dataset-1",
                "title": "Test Dataset 1",
                "description": "PrvnÃ­ testovacÃ­ dataset",
                "keywords": ["test", "data"],
                "themes": ["GOVE"],
                "isHVD": False
            },
            {
                "uri": "https://data.gov.cz/dataset/2", 
                "identifier": "dataset-2",
                "title": "Test Dataset 2",
                "description": "DruhÃ½ testovacÃ­ dataset",
                "keywords": ["example"],
                "themes": ["TECH"],
                "isHVD": True
            }
        ]
    }
    
    current_data = {
        "metadata": {
            "created_at": "2025-01-22T09:00:00",
            "total_datasets": 3,
            "scraper_version": "2.1-test"
        },
        "datasets": [
            {
                "uri": "https://data.gov.cz/dataset/1",
                "identifier": "dataset-1", 
                "title": "Test Dataset 1 - Updated",  # ZmÄ›na!
                "description": "PrvnÃ­ testovacÃ­ dataset s aktualizovanÃ½m popisem",  # ZmÄ›na!
                "keywords": ["test", "data", "updated"],  # ZmÄ›na!
                "themes": ["GOVE"],
                "isHVD": False
            },
            {
                "uri": "https://data.gov.cz/dataset/2",
                "identifier": "dataset-2",
                "title": "Test Dataset 2", 
                "description": "DruhÃ½ testovacÃ­ dataset",
                "keywords": ["example"],
                "themes": ["TECH"],
                "isHVD": True
            },
            {
                # NovÃ½ dataset!
                "uri": "https://data.gov.cz/dataset/3",
                "identifier": "dataset-3",
                "title": "Test Dataset 3 - New",
                "description": "TÅ™etÃ­ testovacÃ­ dataset - zcela novÃ½",
                "keywords": ["new", "fresh"],
                "themes": ["ENVI"],
                "isHVD": False
            }
        ]
    }
    
    return previous_data, current_data


def test_diff_node():
    """TestovacÃ­ funkce pro diff node"""
    print("ğŸ§ª TestovÃ¡nÃ­ JSON Diff Node...")
    
    # VytvoÅ™Ã­ testovacÃ­ data
    previous_data, current_data = create_test_data()
    
    # UloÅ¾Ã­ testovacÃ­ soubory
    test_dir = Path("test_diff_data")
    test_dir.mkdir(exist_ok=True)
    
    prev_path = test_dir / "dataset_previous.json"
    curr_path = test_dir / "dataset_current.json"
    
    with open(prev_path, "w", encoding="utf-8") as f:
        json.dump(previous_data, f, indent=2, ensure_ascii=False)
    
    with open(curr_path, "w", encoding="utf-8") as f:
        json.dump(current_data, f, indent=2, ensure_ascii=False)
    
    # VytvoÅ™Ã­ mock stav
    test_state = ExtendedWeeklyNKODState({
        "previous_dataset_path": str(prev_path),
        "current_dataset_path": str(curr_path)
    })
    
    # SpustÃ­ diff node
    diff_node = JSONDiffNode()
    result_state = diff_node(test_state)
    
    # VypÃ­Å¡e vÃ½sledky
    print("\nğŸ“Š VÃ½sledky porovnÃ¡nÃ­:")
    print(f"Celkem zmÄ›n: {result_state['changes_summary']['total_changes']}")
    print(f"NovÃ© datasety: {result_state['changes_summary']['new_datasets']}")
    print(f"ZmÄ›nÄ›nÃ© datasety: {result_state['changes_summary']['modified_datasets']}")
    print(f"SmazanÃ© datasety: {result_state['changes_summary']['deleted_datasets']}")
    
    print("\nğŸ“ Detaily zmÄ›n:")
    for change in result_state["dataset_changes"]:
        print(f"- {change['change_type'].upper()}: {change['dataset_id']}")
        if change["changed_fields"]:
            print(f"  ZmÄ›nÄ›nÃ¡ pole: {', '.join(change['changed_fields'])}")
    
    print(f"\nğŸ†• PoÄet novÃ½ch/zmÄ›nÄ›nÃ½ch datasetÅ¯ k vrÃ¡cenÃ­: {len(result_state['new_or_modified_datasets'])}")
    
    # VyÄistÃ­ testovacÃ­ soubory
    import shutil
    shutil.rmtree(test_dir)
    
    return result_state


if __name__ == "__main__":
    # NastavenÃ­ loggingu
    logging.basicConfig(level=logging.INFO)
    
    # SpustÃ­ test
    test_diff_node()